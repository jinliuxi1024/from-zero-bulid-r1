# 从零构建语言模型：一份进阶实践指南

> What I cannot create, I do not understand.
>
> —— Richard Feynman

## 写在前面

### 项目背景与目标

随着大语言模型（Large Language Models, LLM）技术的飞速发展，行业范式正从传统的预训练（Pre-training）转向更为直接的推理（Inference）应用。然而，现有的学习资料大多浅尝辄止，导致许多初学者在了解宏观流程后，对于模型架构选择、参数配置、数据集准备等关键细节仍感困惑，难以付诸实践。

同时，当代语言模型对算力的巨大需求，为个人开发者和研究者设置了较高的门槛，在一定程度上限制了技术的下沉与普及。本项目旨在通过提供一个高质量、近工业级别的实践范例（Demo），为后来者铺设一条拾阶而上的道路，使其能更快地投身于未来的技术浪潮。

我们坚信，技术的演进终将化繁为简。昔日复杂庞大的机理，将在时间的淘洗下变得更为凝练，并最终通过量变引发质变。

### 特别说明

本教程定位为**进阶实践**，致力于通过现有开源材料，打磨出一个高性能的模型，并以自顶向下的方式深度剖析实现语言模型的细节，旨在将最有效的部分打磨。

#### 前置知识

为确保学习效果，您需要具备以下基础：
*   基本的语言模型架构知识（如 Transformer）
*   基本的模型训练流程（数据处理、模型定义、训练循环）
*   基本的强化学习概念（如 GRPO）
*   基本的线性代数与高等数学
*   对模型训练并行化（如数据并行、张量并行）有一定了解
*   对模型量化有一定了解
*   对模型推理优化有一定了解

---

## 流程概览

<details>
<summary>点击展开查看完整流程</summary>

*   **模型架构 (Model Architecture)**
    *   注意力机制 (Attention)
    *   多 Token 预测解码 (Multi-Token Prediction)
    *   混合专家模型 (MoE)
    *   全局与交错注意力 (Global & Interleaved Attention)
    *   注意力池化 (Attention Pooling)
*   **分词器 (Tokenizer)**
    *   训练 (Training)
    *   泛化性 (Generalization)
*   **数据集处理 (Dataset Processing)**
    *   中英语料 (Chinese & English Corpora)
    *   推理语料 (Inference Corpora)
    *   冷启动语料 (Cold-Start Corpora)
*   **优化器 (Optimizer)**
    *   AdamW
    *   Muon
*   **参数选择 (Parameter Selection)**
    *   模型参数 (Model Parameters)
    *   训练参数 (Training Parameters)
*   **训练加速 (Training Acceleration)**
    *   混合精度训练 (Mixed-Precision Training)
    *   分布式训练 (Distributed Training)
    *   梯度累积 (Gradient Accumulation)
    *   梯度检查点 (Gradient Checkpointing)
    *   预计算 (Pre-computation)
*   **退火 (Annealing)**
*   **冷启动 (Cold Start)**
*   **强化学习 (Reinforcement Learning)**
    *   GRPO (Generalized Rejection Sampling Policy Optimization)
    *   DAPO (Direct Advantage Policy Optimization)
    *   GSPO (Grafted Supervised Policy Optimization)
*   **指令微调 (Instruction Tuning)**
    *   监督微调 (SFT, Supervised Fine-Tuning)
    *   直接偏好优化 (DPO, Direct Preference Optimization)
*   **性能评估 (Performance Evaluation)**
    *   `lm-eval`
*   **模型量化 (Model Quantization)**
    *   `llama.cpp`
*   **推理部署 (Inference & Deployment)**
    *   `vllm`
    *   `sglang`
    *   `ollama`

</details>

---

## (更新于 2025.9.25)

## 一、模型架构

### 1.1 注意力机制 (Attention Mechanism)

注意力机制（Attention Mechanism）自 Transformer 架构提出以来，特别是随着 ChatGPT-3.5 等模型的广泛应用，已成为现代主流大语言模型的核心构建模块。

从概念上讲，该机制借鉴了数据库的查询（Query）思想。当一个词元（Token）需要从其他词元中获取信息时，我们可以将上下文中的所有词元视为一个小型“数据库”。该词元作为“查询方”（Query），向数据库中的每个词元（作为 Key）发起查询，根据相关性（相似度）得到每个词元所携带信息（Value）的权重，最终将所有 Value 加权求和，完成一次高效的信息提取。

为此，我们将输入序列的隐藏层表示（Hidden States）线性投影成三种不同的向量：查询向量（Query, **Q**）、键向量（Key, **K**）和值向量（Value, **V**），并基于它们进行后续计算。

![Self Attention](images/selfattn.png)

另一方面，我们从上面的计算公式中可以发现，qk相乘还需要除以隐藏层维度的开方，这是因为我们往往初始化q,k在零附近，而两者相乘后，其值域往往与隐藏层成正比，因此，我们需要归一化来避免softmax后的两级化。

##### 多头注意力
引入多头注意力的好处是，将原本庞大的注意力头分解为多个更小的注意力头，从而实现分工合作，提高信息提取的特异性和有效性，在机器可解释性的论文指出，注意力头有归纳推理头，用于上下文复现，也有语法，句法头等等，如果有兴趣可以自行探索。另一方面，当我们把目光放在如何配置模型参数上时，我们需要知道大概多少的注意力维度比较适合，一般来说，注意力维度至少要大于8.33log(N)，N为词向量长度，目前的大语言模型通常使用100k往上的词表预训练，因此96，128是一个常见的选择，这个数学下界在苏剑林老师的文章中有提到。[https://spaces.ac.cn/archives/8711](https://spaces.ac.cn/archives/7695)

自注意力机制不仅是一种强大的特征提取范式，其更核心的优势在于**天然的并行计算特性**。相较于循环神经网络（RNN）或传统卷积神经网络（CNN）等序列依赖模型，Transformer 架构允许在处理序列时一次性计算所有词元之间的关系，极大地提升了数据吞吐量和训练效率。可以说，是这种高效的并行能力，结合海量数据，共同推动了 Transformer 在语言建模任务上的巨大成功。

##### 分组注意力
分组注意力的最初愿景是在计算效率和模型性能之间取得一个比较好的平衡，我们可以这样考虑，当不同的注意力头进行键值查询的时候，能否通过共用同一词向量的数据库进行查询，只是查询的问题不同从而减少对key,value的需求呢，由此可见，分组注意力牺牲了一部分数据的表示而利用一个低秩的共享维度来获取计算效率的提高，因为这个共享的维度将在模型训练过程中共享数据将潜在学会将信息包含其中。

##### 潜在注意力
这种注意力形式，最初被deepseek提出，通过一个巧妙的矩阵转置，将key和value向量压缩到一个低维的投影向量中，通过低秩分解来进一步的推进kv缓存的减少。另外，一个很神奇的地方是在于通过这个实现，潜在注意力可以在推理过程中使用多头注意力的计算方式，而解码推理时却只采用类似分组注意力的方式，换句话说，MLA即有多头注意力的性能，也有分组注意力的解码效率，这是我认为很关键的一点。

#### **实践：基于注意力的图像识别**
为了直观体验注意力机制的原理，我们提供了一个基于全注意力（Vision Transformer）实现手写数字识别的简单示例。

首先，克隆本项目的代码库：
```bash
git clone https://github.com/jinliuxi1024/from-zero-bulid-r1
```
进入项目目录并安装依赖：
```bash
cd from-zero-bulid-r1
pip install -r requirements.txt
```
运行示例代码：
```bash
python attn/tiny_vit.py
```

在这个案例中attn模块基本代替了传统的cnn卷积模块，模型的总体架构是按照数据->特征提取->非线性变换->输出的思路设计，并在几轮后快速收敛。


### 1.2 前馈神经网络 (Feed-Forward Network)

前馈神经网络（FFN）层通常由多层感知机（MLP）构成，在注意力层之后进行非线性变换，旨在对注意力机制提取的特征信息进行深度整合与加工。

近期的研究表明，FFN 层在模型中扮演着至关重要的角色：**它被认为是模型存储和调用其在训练过程中学到的事实性知识（factual knowledge）的关键组件**。人类语言在本质上可能是简洁的，但由于信息不对称（即“噪音”）的存在，使其显得博大精深。知识本身可以被视为对外部世界不确定性的认知与归纳。

模型架构的演进也遵循着从简洁到复杂、再回归到某种“动态简洁”的规律。这引出了一个重要问题：语言建模是否必然需要一个庞大且处处激活的参数集？这正是混合专家模型（MoE）架构试图回答的问题，其核心思想在于，并非所有任务都需要动用模型的全部知识。

### 1.3 混合专家模型 (Mixture of Experts, MoE)

观察语言模型的发展历程，可以发现其架构从相对简洁的 Transformer 设计演变为更具工业化色彩的复杂结构，其中最具代表性的演进便是**混合专家模型（MoE）**的引入。

MoE 最初由 Google 等机构探索，其核心目标是在保持甚至提升模型性能的同时，大幅降低推理时的计算成本。这一架构随后被 Mistral AI (`Mixtral`) 和 DeepSeek (`DeepSeek-MoE`) 等公司发扬光大，并迅速成为当前顶尖大模型的主流设计之一。

其核心思想是：将一个庞大的前馈网络（FFN）层替换为多个小型的“专家”网络（Experts）和一个“路由器”（Router）。在处理每个输入词元时，路由器会动态地、有选择性地激活一小部分（通常是 1-2 个）专家来参与计算，而其他专家则保持休眠。

这种**稀疏激活（Sparse Activation）**的特性带来了显著的经济性。然而，对于参数规模相对较小的模型，我们应如何考量 MoE 的应用呢？

结论是肯定的。尽管 MoE 模型的性能无法完全等同于一个参数量为其**总参数量**的密集模型（Dense Model），但其实际性能通常远超一个参数量与其**激活参数量**相等的密集模型。它通过在巨大的参数空间中进行稀疏选择，实现了性能与计算效率之间的卓越平衡。

#### 1.3.1 共享专家架构
在deepseekv2中，论文认为，语言建模的部分知识是可以共享的，对于专家分工的考虑上，应当实现精细化的设置，即几个共享专家，和庞大的路由专家公共协作完成任务，这与我们的直觉一致，回答特定问题的一些知识是类似的，比如，当我们回答语言模型或者视觉模型的架构时，我们总体对神经网络的建模知识总是作为共享的部分参与回答的组成，降级来说，即使我们回答今天吃什么或者你好本身来说，两者不相关的问题也存在句法或者词法的基本知识，使我们的回答可以被对方理解。

### 1.4 多token预测解码
